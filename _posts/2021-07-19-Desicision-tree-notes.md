---
layout:     post
title:      "决策树学习笔记"
subtitle:   "Study Notes for Decision Tree"
date:       2021-07-19 08:00:00
author:     "Huang"
catalog: false
header-style: text
tags:
  - 机器学习
  - 学习笔记
---
# ML-决策树

决策树模型适合分类和回归

- 分类： 每个子节点代表一个类别
- 回归：根据特征向量来决定对应的输出值，回归树将特征空间划分为若干个单元，每一个划出的单元有一个特定的输出，每个单元对应一个输出值

# 构建树的过程

1. 特征选择：树节点的分裂标准，不同标准对应着不同的决策算法，如ID3，C4.5，CART
2. 生成决策树：根据特征评估标准，从上至下生成子节点，知道决策树停止生长
3. 剪枝处理「Pruning」：防止“过拟合”的主要手段，主动去掉一些分支降低过拟合风险，剪枝策略又分有预剪枝和后剪枝

## 决策树评估标准

**信息熵：一种衡量样本集纯度的指标**

$$Ent(D) = \red{-}\sum_{k=1}^{|\mathcal{Y}|}p_klog_2p_k$$

$|\mathcal{Y}|$ 为类别集合，$p_k$为该类样本占样本总数的比例。

信息熵越大，表示样本集的混乱程度越高，纯度越低

### 信息增益「info entropy」- ID3「Iterative Dichotomiser」决策树

**信息增益（information gain）是ID3采用的节点划分规则**

$$Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)$$

它描述的是按某种属性划分后纯度的提升，信息增益越大，代表用属性$a$进行划分所获得的纯度提升越大。

$V$表示属性 $a$ 的属性值集合，$D^v$表示属性值为$v$ 的数据子集。

**求和项**也称为条件熵，我们可以理解为它是先求出每个数据子集的信息熵，然后按每个数据子集占原数据集的比例来赋予权重，比例越大，对提升纯度的帮助就越大。

当多个属性都取得最大的信息增益时，任取其中一个即可。
信息增益又称为**互信息（Mutual information）：**

- 一个连续变量X的不确定性，用方差Var(X)来度量
- 一个离散变量X的不确定性，用熵H(X)来度量
- 两个连续变量X和Y的相关度，用**协方差**或**相关系数**来度量
- 两个离散变量X和Y的相关度，用**互信息I(X;Y)**来度量(直观地，X和Y的相关度越高，X对分类的作用就越大)

### 增益率「gain ratio」- C4.5 决策算法

$$\begin{equation}
Gain_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}
\end{equation}\\
\begin{equation}
IV(a) = \red-\sum_{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
\end{equation}$$

$IV$称为属性的固有值（intrinsic value），它的定义和信息熵是类似的，信息熵衡量的是样本集在类别上的混乱程度，而**固有值衡量的是样本集在某个属性上的混乱程度**。固有值越大，则该属性混乱程度越高，可能的取值越多。

**增益率**是为了避免模型过份偏好用取值多的属性作划分。这是使用信息增益作准则非常容易陷入的误区，比方说每个样本都有一个“编号”属性，这个属性的条件熵肯定是最小的，但如果选择了该属性作为根节点，那么构建出的决策树就没有任何意义了，因为这个模型根本不具备泛化性能。

> **注意！**C4.5并非直接选择增益率最高的属性，它使用了一个启发式：先从属性集中找到信息增益高于平均水平的属性作为候选，然后再比较这些候选属性的增益率，从中选择增益率最高的。

### 基尼指数「Gini index」- CART决策

**基尼值：$Gini(D) = \sum_{k=1}^{|\mathcal{Y}|}\sum_{k' \neq k}p_kp_{k'}\ =1-\sum_{k=1}^{|\mathcal{Y}|}p_k^2$**

基尼值是另一种衡量样本集纯度的指标。反映的是从一个数据集中**随机**抽取**两个样本**，其类别标志不同的**概率**。

**基尼值越小，样本集的纯度越高。**

**基尼指数：$Gini\_index(D,a) = \sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)$**

由基尼值引伸开来的就是基尼指数这种准则了，基尼指数越小，表示使用属性$a$划分后纯度的提升越大。

在属性集合$A$ 中，选择基尼指数最小的属性作为最优划分属性，

即$a_*=\operatorname{\argmin}_{a\isin A}Gini\_index(D,a)$

### 3大算法总结：

从实现细节、优化过程等角度，这三种决策树还有一些不同。

- ID3对样本特征缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处理;
- ID3和C4.5可以在每个结点上产生出多叉分支，且每个特征在层级之间不会复用，而CART每个结点只会产生两个分支，因此最后会形成一颗二叉树，且每个特征可以被重复使用;
- ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，而CART直接利用全部数据发现所有可能的树结构进行对比。

## 如何对决策树进行剪枝

### 预剪枝

在决策树生成过程中，对每个节点在划分前进行估计，如果当前节点的划分不能提高决策树的泛化能力，则停止划分，并将此节点标记为叶节点。

> 预剪枝是一种贪心策略，因为它在决策树生成时就杜绝了很多分支展开的机会。
**优点：**不但降低了过拟合的风险，同时也显著减少了模型的训练时间开销和测试时间开销。
**缺点：**这种贪心策略有可能导致欠拟合，因为有可能当前划分不能提升模型的泛化性能，但其展开的后续划分却会显著提升泛化性能。在预剪枝中这种可能被杜绝了。

### 后剪枝

先从训练集中生成一个完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点地换位叶节点能带来决策树泛化性能提升，则将此节点替换为叶节点。

> 后剪枝是种比较保守的策略.
**优点：**欠拟合的风险很小，泛化性能往往优于预剪枝的决策树。
**缺点：**由于后剪枝是在生成了完整决策树后，自底向上对所有非叶节点进行考察，所以训练时间开销要比未剪枝决策树和预剪枝决策树都大得多。

常见后剪枝方法：错误率降低剪枝(Reduced Error Pruning，REP)、悲观剪枝(Pessimistic Error Pruning，PEP)、代价复杂度剪枝(Cost Complexity Pruning，CCP)、最小误差剪枝(Minimum Error Pruning，MEP)、CVP(Critical Value Pruning)、OPP(Optimal Pruning)等方法，这些剪枝方法各有利弊，关注不同的优化角度

### 代价剪枝的2个步骤：代添加

### 如何判断剪枝有没有用呢？

具体来说就是判断剪枝后模型的泛化性能有没有提升？这就涉及到第二章模型评估与选择的内容了。不过这里不用做比较检验。我们需要做的是：

1. 首先是选定一种评估方法划分训练集和测试集
2. 然后选定一种性能度量用来衡量剪枝前后的模型在测试集上的效果。

> **奥卡姆剃刀定律(Occam’s Razor，Ockham’s Razor):
「如无必要，勿增实体 / 简单有效原理」
误区：简单 ⇒ 正确，此定律通常表达为：当两个假说具有完全相同的解释力和预测力时，我们以那个较为简单的假说作为讨论依据**
奥卡姆剃刀定律与机器学习消除过拟合的思想一致，在预测力不减的同时，用一个简单的模型去代替原来的复杂模型，类似的思想还有神经网络中的Dropout的方法

## 连续值与缺失值

### 连续值：

前面线性模型已经谈到了离散属性连续化，而决策树模型需要的则是**连续属性离散化**，因为决策树**每次判定只能做有限次划分**。最简单的一种离散化策略是C4.5算法采用的**二分法（bi-partition）**。

给定一个包含连续属性 $a$ 的数据集，并且 $a$ 在数据集中有 $n$ 个不同取值，我们先把属性 $a$ 的 $n$ 个属性值**从小到大进行排序**。**所谓“二分”是指将这些属性值分为两个类别**（比方说把身高这一属性分为高于170和低于170两个类别）。

这就产生了一个新问题，怎么找到合适的划分点（例如上面例子的170）呢？

在对连续属性值排序完之后，由于有 $n$ 个不同取值，取每**两个取值的平均值作为划分点**的话，就有 $n-1$ 个候选划分点。我们需要做得就是按照准则（比方说用ID3算法的话就是信息增益）进行 $n-1$ 次判断。每次拿出一个候选划分点，把连续属性分为两类，转换为离散属性。然后基于这个基础计算准则，最终选出一个最优的属性值划分点。

$$\begin{align*}
Gain(D,a) = \max_{t\isin T_a}{Gain(D,a,t)}
\end{align*}\\
\begin{align*}
= \max_{t\isin T_a}Ent(D)-\sum_{\lambda\isin{+,-}}\frac{|D^\lambda_t|}{|D|}Ent(D^\lambda_t)
\end{align*}
$$

注意！和离散属性不同，连续属性用于当前节点的划分后，其**后代节点依然可以使用该连续属性进一步划分**。比方说当前节点用身高低于170划分了，那么它的后代节点还可以用身高低于160来进一步划分。

### 缺失值

如何样本中有大量缺失值，如果直接删除，则使用无缺失值的样本来学习是对数据信息的极大浪费。面对缺失值的数据，该如何学习？

2大问题：

1. **如何在属性值有缺失的情况下进行划分属性的选择**
2. **给定划分属性，若样本在该属性上有缺失，改如何划分**

### 解决问题1：**在属性值有缺失的情况下进行划分属性的选择**

假设数据集为 $D$，有缺失值的属性为  $a$，令 $\tilde{D}$ 表示 $\tilde{D}$ 中没有缺失属性 $a$ 的样本子集。我们只能基于 $\tilde{D}$ 来判断属性 $a$ 的优劣。但是我们又希望包含缺失值的样本也能在建模过程体现出一定的影响了，因此要重新定义准则。在那之前，先定义几个新定义用到的变量：

$$\begin{align*}
\rho = \frac{\sum_{\mathbf{x} \in \tilde{D}}w_\mathbf{x}}{\sum_{\mathbf{x} \in D}w_\mathbf{x}}
\end{align*}\\
\begin{align*}
\tilde{p_k} = \frac{\sum_{\mathbf{x} \in \tilde{D_k}}w_\mathbf{x}}{\sum_{\mathbf{x} \in \tilde{D}}w_\mathbf{x}},\quad (1 \leq k \leq |\mathcal{Y}|)
\end{align*}\\
\begin{align*}
\tilde{r_v} = \frac{\sum_{\mathbf{x} \in \tilde{D^v}}w_\mathbf{x}}{\sum_{\mathbf{x} \in \tilde{D}}w_\mathbf{x}},\quad (1 \leq v \leq V)
\end{align*}$$

 $\rho$  表示**无缺失值样本所占的比例**;

$\tilde{p_k}$ 表示**无缺失值样本中第** $k$ **类所占的比例**;

$\tilde{r_v}$ 表示**无缺失值样本中在属性** $a$ **上取值** $a^v$ **的样本所占的比例** ;

注意，这里的 $w_\mathbf{x}$ 表示样本的权值，它是**含缺失值样本参与建模**的一种方式。在根节点处初始时，所有样本 $\mathbf{x}$ 的权重都为1。

---

**重新定义信息熵和信息增益：**

$$\begin{align*}
Ent(\tilde{D}) = -\sum_{k=1}^{|\mathcal{Y|}}\tilde{p_k}log_2\tilde{p_k}
\end{align*}\\
\begin{align*}
Gain(D,a) = \rho \times Gain(\tilde{D},a)\ = \rho \times (Ent(\tilde{D}) - \sum_{v=1}^V\tilde{r_v}Ent(\tilde{D^v}))
\end{align*}
$$

按照新的定义来计算包含缺失值的属性的信息增益，然后和其他属性的信息增益相比，选出最优划分属性

### 解决问题2：**给定划分属性，若样本在该属性上有缺失，改如何划分**

假设有一个包含缺失值的属性被计算出是最优划分属性，那么我们就要按该属性的不同取值划分数据集了。缺失该属性值的样本怎么划分呢？答案是按**概率划分**，这样的样本会被同时划入所有子节点，并且其**权重更新**为对应的 $\tilde{r_v} \dot w_\mathbf{x}$。让同一个样本以不同的概率划入到不同的子节点中。

可以把无缺失值的决策树建模想象为各样本权值恒为1的情形，它们只对自己所属的属性值子集作贡献。而样本含缺失值时，它会以不同的概率对所有属性值子集作贡献。

### 决策树的优缺点：

**优点:**

- 简单直观，生成的决策树很直观。
- 基本不需要预处理，不需要提前归一化，处理缺失值。
- 使用决策树预测的代价是𝑂(𝑙𝑜𝑔2𝑚)。 m为样本数。
- 既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。
- 可以处理多维度输出的分类问题。
- 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释
- 可以交叉验证的剪枝来选择模型，从而提高泛化能力。
- 对于异常点的容错能力好，健壮性高。

**缺点：**

- 决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。
- 决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。
- 寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。
- 有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。
- 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。

# 决策树Q&A

- 1. 决策树为什么容易过拟合

    决策树的生成过程中，通过不断分支，将样本实例划分到合适的单元，当样本中存在噪声，即可能是特征值观测误差或者标签值观测误差，使在分支归节点的时候产生矛盾，这时决策树选择继续生成新的分支，来产生更加“完美”的叶子节点，这便是由于噪音数据带来的误生成的分支，使得训练变得更加优越，而泛化能力下降

- 2. 决策树的深浅和对应的条件概率模型有何关系

    每条路径后的叶子节点对应着特征空间的一个划分区域,而此区域内估计各类的概率,便是此路径下的条件概率,当决策树模型较浅时,对应的路径上的节点数也较少,从而概率路径上的特征也较少,这表示,通过较少的特征估计了所有特征组合里的众多可能的条件概率,因此,较浅的决策树对应着舍弃某些特征组合下的泛条件概率模型(参数复杂度低)

- 3.信息增益倾向于选择取值较多的特征，为何？

    信息增益在计算的过程中，存在对某个特征的某取值时的数据集合内的各类概率估计，当该特征的取值较多时，分到每个值小面的样本数也会少一些，而这使得概率的估计的稳定性变差（或者说离大数定律的要求越远），使得估计出的概率容易出现非均匀的情况，从而造成条件熵下降，即信息增益变大的倾向，但不是所有情况下都是这样的，当数据集非常大，或者说那些取值多的特征并没有多到很夸张时，信息增益并没有多大偏向性。

- 4. 信息增益率如何消除信息增益的倾向性？

    通过将信息增益值与特征的内部熵值比较，消除因为特征取值较多带来的概率估计偏差的影响。其本质是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。这带来一个新的问题是，倾向于选择特征取值少的。

- 5.CART中的回归树在生成过程中，特征会重复出现吗？树生成的停止条件是啥？

    特征会复用，停止的条件是基尼指数低于阈值，或者样本数太少没有分支的意义，再或者是没有特征可供选择。补充：ID3和C4.5的特征不会复用，且是多分叉的树。

- 6.决策树出现过拟合的原因
    1. 决策树构建过程中,对决策树生长没有进行合理的限制
    2. 在建模过程中使用了较多的输出变量
    3. 样本中有一些噪声数据,噪声数据对决策树的构建干扰很多