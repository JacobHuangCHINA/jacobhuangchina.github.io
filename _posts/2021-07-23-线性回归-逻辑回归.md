---
layout:     post
title:      "线性回归、逻辑回归学习笔记"
subtitle:   "Study Notes for Regression"
date:       2021-07-23 08:00:00
author:     "Huang"
catalog: false
header-style: text
tags:
  - 机器学习
  - 学习笔记
---
# ML-线性回归、逻辑回归

# 线性回归

给定由d个属性描述示例$x=(x_1;x_2;x_3;...;x_d)$, 线性模型学习一个通过属性的线性组合来进行预测函数如下：

$$f(x)=w_1x_1+w_2x_2+w_3x_3+...+w_dx_d+b$$

一般用向量形式写成：$其中w=(w_1;w_2;w_3;...w_d)，确定w和b之后，模型可确定$

$$f(x) = w^Tx + b
$$

### 如何确定w和b呢

关键在于如何衡量$f(x)$与$y$之间的差别。而均方误差是回归任务中常见的性能度量，是均方误差最小化，即：

$$\begin{align*}
   (w^*,b^*)/L(f(x_i)) = \argmin _{{(w,b)}}{\sum _{i=1}^{n}(f(x_i)-y_{i})^{2}} \\ = \argmin _{{(w,b)}}{\sum _{i=1}^{n}(y_i-wx_i-b)^{2}}
\end{align*}$$

均方误差对应了常用的欧几里得距离「欧式距离-Euclidean distance」基于均方误差最小化来进行模型求解的方法称为“最下二乘法”「在线性回归中，最下二乘法就是试图找到一条之休闲，使所有样本到直线上的欧式距离之和最小」

公式推理ref:[https://datawhalechina.github.io/pumpkin-book/#/chapter3/chapter3?id=_35](https://datawhalechina.github.io/pumpkin-book/#/chapter3/chapter3?id=_35)

求解w和b，使$E_{(w,b)}=\sum_{i=1}^{n}(y_i-wx_i-b)^2$最小化过程，称为线性回归模型的最小二乘「参数估计-parameter estimation」，对w和b分别求导，并且令导数为0，则可以得到两个参数的闭式「close-form」解「解析解」：

$$\begin{align}
w = \frac{\sum_{i=1}^m y_i(x_i - \bar{x})}{\sum_{i=1}^m x_i^2 - \frac{1}{m}(\sum_{i=1}^m x_i)^2} \\ b = \frac{1}{m} \sum_{i=1}^m (y_i-wx_i)
\end{align}
$$

> 为什么可以这样求解呢？
因为损失函数是一个凸函数（记住是向下凸，类似U型曲线），导数为0表示该函数曲线最低的一点，此时对应的参数值就是能使均方误差最小的参数值。特别地，要判断一个函数是否凸函数，可以求其二阶导数，若二阶导数在区间上非负则称其为凸函数，若在区间上恒大于零则称其为严格凸函数。

### 多元线性回归

样例包含多种属性d，则需要医用多元线性回归。 把w和b转化为向量形式$\mathbf{\hat{w}} = (\mathbf{w};b)$，将数据集D表示为一个$x\times(d+1)$大小的矩阵X，其中每行对应一示例，改行前d个元素对应d个属性，最后一个元素恒为1, 此时损失函数如下：

$$E_{\mathbf{\hat{w}}} = (\mathbf{y} - X\mathbf{\hat{w}})^T (\mathbf{y} - X\mathbf{\hat{w}})$$

使用最小二乘法进行参数估计，对$\mathbf{\hat{w}}$进行求导：

$$\frac{\partial E_{\mathbf{\hat{w}}}}{\partial \mathbf{\hat{w}}} = 2 X^T(X\mathbf{\hat{w}} - \mathbf{y})$$

最后得到闭式解：

$$\mathbf{\hat{w}}* = (X^TX)^{-1}X^T\mathbf{y}$$

# 逻辑回归

# 正则化项L1和L2 「Regularization」

- ref

    [https://www.cnblogs.com/zingp/p/10375691.html](https://www.cnblogs.com/zingp/p/10375691.html)

L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）

L1

$\min_w [\sum_{i=1}^{N}(w^Tx_i - y_i)^2 + \lambda \|w\|_1 ]........(1)$

- 是模型各个参数的绝对值之和；
- 会趋向于产生少量的特征，而其他的特征都是0，产生稀疏权重矩阵；

L2

$\min_w[\sum_{i=1}^{N}(w^Tx_i - y_i)^2 + \lambda\|w\|_2^2] ........(2)$

- 是模型各个参数的平方和的开方值。
- L2会选择更多的特征，这些特征都会接近于0，当最小化||w||时，就会使每一项趋近于0，防止过拟合。

## 正则化的作用：

- L1正则化可以使得参数稀疏化，即得到的参数是一个稀疏矩阵，可以用于特征选择。
    - 稀疏性，说白了就是模型的很多参数是0。通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，很多参数是0，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，即使去掉对模型也没有什么影响，此时我们就可以只关注系数是非零值的特征。这相当于对模型进行了一次特征选择，只留下一些比较重要的特征，提高模型的泛化能力，降低过拟合的可能。
- L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合。

## L1 正则化与稀疏性

事实上，”带正则项”和“带约束条件”是等价的。

为了约束w的可能取值空间从而防止过拟合，我们为该最优化问题加上一个约束，就是w的L1范数不能大于m：

$$\begin{cases}
   \min \sum_{i=1}^{N}(w^Tx_i - y_i)^2 \\
   s.t. \|w\|_1 \leqslant m.
\end{cases}........(3)$$

问题转化成了带约束条件的凸优化问题，写出拉格朗日函数:

$$\sum_{i=1}^{N}(w^Tx_i - y_i)^2 + \lambda (\|w\|_1-m)........(4)$$

设𝑊∗和𝜆∗是原问题的最优解，则根据𝐾𝐾𝑇条件得

$$\begin{cases}
   0 = \nabla_w[\sum_{i=1}^{N}(W_*^Tx_i - y_i)^2 + \lambda_* (\|w\|_1-m)] \\
   0 \leqslant \lambda_*.
\end{cases}........(5)$$

仔细看上面第一个式子，与公式(1)其实是等价的，等价于(3)式。

设L1正则化损失函数：$J = J_0 + \lambda \sum_{w} |w|$，其中$J_0 = \sum_{i=1}^{N}(w^Tx_i - y_i)^2$是原始损失函数，加号后面的一项是L1正则化项，𝜆是正则化系数。

**1.为什么参数越小代表模型越简单？**

越是复杂的模型，越是尝试对所有样本进行拟合，包括异常点。这就会造成在较小的区间中产生较大的波动，这个较大的波动也会反映在这个区间的导数比较大。只有越大的参数才可能产生较大的导数。因此参数越小，模型就越简单。

**2.实现参数的稀疏有什么好处？**

因为参数的稀疏，在一定程度上实现了特征的选择。一般而言，大部分特征对模型是没有贡献的。这些没有用的特征虽然可以减少训练集上的误差，但是对测试集的样本，反而会产生干扰。稀疏参数的引入，可以将那些无用的特征的权重置为0.

**3.L1范数和L2范数为什么可以避免过拟合？**

加入正则化项就是在原来目标函数的基础上加入了约束。当目标函数的等高线和L1,L2范数函数第一次相交时，得到最优解。

**L1范数-Lasso：**

$\min_w [\sum_{i=1}^{N}(w^Tx_i - y_i)^2 + \lambda \|w\|_1 ]........(1)$

L1范数符合拉普拉斯分布，是不完全可微的。表现在图像上会有很多角出现。这些角和目标函数的接触机会远大于其他部分。就会造成最优值出现在坐标轴上，因此就会导致某一维的权重为0 ，产生稀疏权重矩阵，进而防止过拟合。

**L2范数-Ridge：**

$\min_w[\sum_{i=1}^{N}(w^Tx_i - y_i)^2 + \lambda\|w\|_2^2] ........(2)$

L2范数符合高斯分布，是完全可微的。和L1相比，图像上的棱角被圆滑了很多。一般最优值不会在坐标轴上出现。在最小化正则项时，可以是参数不断趋向于0.最后活的很小的参数。

## 如何设置正则化项的参数

- 以L2正则化参数为例：从公式(8)可以看到，λ越大，𝜃𝑗衰减得越快。另一个理解可以参考L2求解图， 𝜆越大，L2圆的半径越小，最后求得代价函数最值时各参数也会变得很小；当然也不是越大越好，太大容易引起欠拟合。
- **经验**
    - 从0开始，逐渐增大。在训练集上学习到参数，然后在测试集上验证误差。反复进行这个过程，直到测试集上的误差最小。一般的说，随着从0开始增大，测试集的误分类率应该是先减小后增大，交叉验证的目的，就是为了找到误分类率最小的那个位置。
    - 建议一开始将正则项系数λ设置为0，先确定一个比较好的learning rate。然后固定该learning rate，给一个值（比如1.0），然后根据validation accuracy，将λ增大或者减小10倍，增减10倍是粗调节，当你确定了的合适的数量级后，比如，再进一步地细调节，比如调节为0.02，0.03，0.009之类。